{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Segundo parcial SIS 420**\n",
        "\n",
        "**Nombre: Yavo Chavez Abigail Zulma**\n",
        "\n",
        "**Carrera: Ingenieria en diseño y animación digital**\n",
        "\n",
        "##**4 EN RAYA**\n",
        "\n",
        "**El aprendizaje** por refuerzo es un método en el que un agente aprende a tomar decisiones para maximizar una recompensa numérica. El agente descubre qué acciones le darán la mayor recompensa a través de la interacción con su entorno. Este método se diferencia del aprendizaje supervisado y no supervisado, ya que el agente no siempre tiene ejemplos del comportamiento deseado y debe aprender de su propia experiencia. El principal desafío es encontrar un equilibrio entre explorar nuevas acciones y explotar las acciones conocidas. El aprendizaje por refuerzo considera todo el problema en su conjunto, en lugar de resolver pequeñas partes por separado.\n",
        "\n",
        "\n",
        "**El sistema de aprendizaje por refuerzo (AxR) consta de cuatro elementos esenciales:**\n",
        "\n",
        "1. Política: Define el comportamiento del agente en relación con los estados que percibe y las acciones que puede tomar.\n",
        "\n",
        "2. Recompensa: Es un valor numérico que el entorno envía al agente, y el objetivo del agente es maximizar esta recompensa.\n",
        "\n",
        "3. Función de valor: Indica la calidad a largo plazo de un estado, considerando la cantidad total de recompensa que se espera acumular en el futuro empezando en ese estado.\n",
        "\n",
        "4. Modelo del entorno: Imita el comportamiento del entorno y sirve para planificar acciones considerando estados futuros que todavía no se han experimentado.\n",
        "\n",
        "Estos elementos son fundamentales para que el agente aprenda a tomar decisiones para maximizar su recompensa a través de la interacción con su entorno.\n",
        "\n"
      ],
      "metadata": {
        "id": "_B9Vb_AbJj0O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni0qNIU9QDCJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Board():\n",
        "    def __init__(self):\n",
        "        # Inicializa el tablero del juego como una matriz de ceros 4x4\n",
        "        self.state = np.zeros((4, 4))\n",
        "\n",
        "    def valid_moves(self):\n",
        "        # Devuelve una lista de tuplas que representan las posiciones válidas en el tablero donde se puede colocar un símbolo\n",
        "        return [(i, j) for j in range(4) for i in range(4) if self.state[i, j] == 0]\n",
        "\n",
        "    def update(self, symbol, row, col):\n",
        "        # Actualiza el estado del tablero con el símbolo (1 para X, -1 para O) en la fila y columna especificadas\n",
        "        if self.state[row, col] == 0:\n",
        "            self.state[row, col] = symbol\n",
        "        else:\n",
        "            raise ValueError(\"¡Movimiento ilegal!\")\n",
        "\n",
        "    def is_game_over(self):\n",
        "        # Comprueba filas y columnas\n",
        "        for i in range(4):\n",
        "            if abs(sum(self.state[i, :])) == 4:\n",
        "                return np.sign(sum(self.state[i, :]))\n",
        "            if abs(sum(self.state[:, i])) == 4:\n",
        "                return np.sign(sum(self.state[:, i]))\n",
        "\n",
        "        # Comprueba diagonales\n",
        "        diag1 = sum([self.state[i, i] for i in range(4)])\n",
        "        diag2 = sum([self.state[i, 3 - i] for i in range(4)])\n",
        "\n",
        "        if abs(diag1) == 4:\n",
        "            return np.sign(diag1)\n",
        "        if abs(diag2) == 4:\n",
        "            return np.sign(diag2)\n",
        "\n",
        "         # Comprueba si hay un ganador\n",
        "        for value in [diag1, diag2] + [self.state.sum(axis=0).sum(), self.state.sum(axis=1).sum()]:\n",
        "          if abs(value) == 4:\n",
        "            return np.sign(value)\n",
        "\n",
        "\n",
        "        # Comprueba si hay empate\n",
        "        if len(self.valid_moves()) == 0:\n",
        "            return 0\n",
        "\n",
        "        # Continuar el juego\n",
        "        return None\n",
        "\n",
        "    def reset(self):\n",
        "        # Resetea el tablero del juego a su estado inicial (todos los ceros)\n",
        "        self.state = np.zeros((4, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMkWNq1rQDCL"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class Game():\n",
        "    def __init__(self, player1, player2):\n",
        "        # Asigna los símbolos de los jugadores (1 para player1 y -1 para player2)\n",
        "        player1.symbol = 1\n",
        "        player2.symbol = -1\n",
        "        # Almacena los jugadores y crea un tablero\n",
        "        self.players = [player1, player2]\n",
        "        self.board = Board()\n",
        "\n",
        "    # selfplay: función para que los jugadores jueguen entre sí\n",
        "    def selfplay(self, rounds=100):\n",
        "        # Inicializamos una lista para registrar las victorias de cada jugador\n",
        "        wins = [0, 0]\n",
        "\n",
        "        # Itera sobre el número de rondas especificadas\n",
        "        for i in tqdm(range(1, rounds + 1)):\n",
        "            # Resetea el tablero y los jugadores para una nueva partida\n",
        "            self.board.reset()\n",
        "\n",
        "            for player in self.players:\n",
        "                player.reset()\n",
        "\n",
        "            game_over = False\n",
        "\n",
        "            # Bucle principal del juego\n",
        "            while not game_over:\n",
        "                for player in self.players:\n",
        "                    # Cada jugador realiza un movimiento en el tablero\n",
        "                    action = player.move(self.board)\n",
        "                    self.board.update(player.symbol, action[0], action[1])\n",
        "\n",
        "                    # Actualiza el estado del jugador después del movimiento\n",
        "                    for player in self.players:\n",
        "                        player.update(self.board)\n",
        "\n",
        "                    # Verifica si el juego ha terminado\n",
        "                    if self.board.is_game_over() is not None:\n",
        "                        game_over = True\n",
        "                        break\n",
        "\n",
        "            # Proporciona la recompensa al finalizar la partida\n",
        "            self.reward()\n",
        "\n",
        "            # Registra las victorias de cada jugador\n",
        "            for ix, player in enumerate(self.players):\n",
        "                if self.board.is_game_over() == player.symbol:\n",
        "                    wins[ix] += 1\n",
        "        return wins\n",
        "\n",
        "    # reward: función para proporcionar recompensas a los jugadores al final de una partida\n",
        "    def reward(self):\n",
        "        # Determina el ganador del juego\n",
        "        winner = self.board.is_game_over()\n",
        "\n",
        "        if winner == 0: # Si hay empate, ambos jugadores reciben una recompensa de 0.5\n",
        "            for player in self.players:\n",
        "                player.reward(0.5)\n",
        "        else:\n",
        "            # Si hay un ganador, el jugador que ganó recibe una recompensa de 1, mientras que el otro recibe 0\n",
        "            for player in self.players:\n",
        "                if winner == player.symbol:\n",
        "                    player.reward(1)\n",
        "                else:\n",
        "                    player.reward(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV0zhZ_4QDCM"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, alpha=0.5, prob_exp=0.55):\n",
        "        # Inicializa la función de valor como un diccionario vacío\n",
        "        self.value_function = {}  # tabla con pares estado -> valor\n",
        "        self.alpha = alpha  # tasa de aprendizaje\n",
        "        # Inicializa la lista de posiciones para almacenar los estados del juego\n",
        "        self.positions = []  # guardamos todas las posiciones de la partida\n",
        "        self.prob_exp = prob_exp  # probabilidad de exploración\n",
        "\n",
        "    def reset(self):\n",
        "        # Resetea la lista de posiciones al comienzo de cada partida\n",
        "        self.positions = []\n",
        "\n",
        "    # Funcion move: función para que el agente realice un movimiento en el tablero dado\n",
        "    def move(self, board, explore=True):\n",
        "        # Obtiene las posibles jugadas válidas en el tablero\n",
        "        valid_moves = board.valid_moves()\n",
        "\n",
        "        # EXPLORACIÓN\n",
        "        if explore and np.random.uniform(0, 1) < self.prob_exp:\n",
        "            # Se elige una posición aleatoria si se está explorando\n",
        "            ix = np.random.choice(len(valid_moves))\n",
        "            return valid_moves[ix]\n",
        "\n",
        "        # EXPLOTACIÓN\n",
        "        # Se elige la posición con el valor más alto en la función de valor\n",
        "        max_value = -1000\n",
        "        for row, col in valid_moves:\n",
        "            next_board = board.state.copy()\n",
        "            next_board[row, col] = self.symbol\n",
        "            next_state = str(next_board.reshape(4*4))\n",
        "\n",
        "            value = 0 if self.value_function.get(next_state) is None else self.value_function.get(next_state)\n",
        "\n",
        "            if value >= max_value:\n",
        "                max_value = value\n",
        "                best_row, best_col = row, col\n",
        "        return best_row, best_col\n",
        "\n",
        "    def update(self, board):\n",
        "        # Almacena el estado actual del tablero en la lista de posiciones\n",
        "        self.positions.append(str(board.state.reshape(4*4)))\n",
        "\n",
        "    # Obtiene la recompensa del estado actual\n",
        "    def reward(self, reward):\n",
        "        # Actualiza los valores de la función de valor al final de la partida\n",
        "        # Itera sobre los estados de la partida en orden inverso\n",
        "        for p in reversed(self.positions):\n",
        "            # Si el estado no está en la función de valor, lo inicializa a 0\n",
        "            if self.value_function.get(p) is None:\n",
        "                self.value_function[p] = 0\n",
        "\n",
        "            # Actualiza el valor del estado utilizando la fórmula de actualización\n",
        "            self.value_function[p] += self.alpha * (reward - self.value_function[p])\n",
        "            # Actualiza la recompensa para el próximo estado\n",
        "            reward = self.value_function[p]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkGvIkfQQDCN",
        "outputId": "78a1677d-328c-42db-9ad3-1e2cd447ac1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [00:11<00:00, 36.06it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[137, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Se crea una instancia del agente 1 con una probabilidad de exploración del 55%\n",
        "Agent1 = Agent(prob_exp=0.55)\n",
        "# Se crea una instancia del agente 2 con la configuración predeterminada (probabilidad de exploración 0.5)\n",
        "Agent2 = Agent()\n",
        "\n",
        "# Se crea una instancia del juego con los dos agentes creados anteriormente\n",
        "game = Game(Agent1, Agent2)\n",
        "\n",
        "# Los agentes juegan entre sí un número especificado de rondas (400 en este caso)\n",
        "game.selfplay(400)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ordenamos la función de valor del Agente1 en orden descendente según los valores\n",
        "funcion_de_valor = sorted(Agent1.value_function.items(), key=lambda kv: kv[1], reverse=True)\n",
        "\n",
        "# Creamos un DataFrame de pandas a partir de la función de valor ordenada\n",
        "# con dos columnas: 'Estado' que contiene los estados del juego y 'Valor' que contiene los valores asociados\n",
        "tabla = pd.DataFrame({'Estado': [x[0] for x in funcion_de_valor], 'Valor': [x[1] for x in funcion_de_valor]})\n",
        "\n",
        "# Mostramos la tabla\n",
        "print(tabla)"
      ],
      "metadata": {
        "id": "2z1n8PcBRGff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5135e0b-314f-4fc4-add2-efa262888251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Estado  Valor\n",
            "0     [-1.  0.  0.  1.  1.  1.  1.  1.  0. -1.  1. -...   0.75\n",
            "1     [ 1. -1. -1.  1.  0. -1. -1. -1.  1.  1.  1.  ...   0.50\n",
            "2     [ 0.  1. -1.  1.  0. -1.  1. -1.  0.  1. -1.  ...   0.50\n",
            "3     [-1.  0.  1. -1. -1.  0.  0.  1.  1.  1.  1.  ...   0.50\n",
            "4     [ 0.  0.  0.  0.  1.  1.  1.  1.  1.  0. -1. -...   0.50\n",
            "...                                                 ...    ...\n",
            "4759  [ 0. -1. -1. -1. -1. -1.  1.  1.  0.  0.  1.  ...   0.00\n",
            "4760  [ 0. -1. -1. -1.  0. -1.  1.  1.  0.  0.  1.  ...   0.00\n",
            "4761  [ 0. -1. -1. -1.  0. -1.  1.  1.  0.  0.  1.  ...   0.00\n",
            "4762  [ 0.  0. -1. -1.  0. -1.  1.  1.  0.  0.  1.  ...   0.00\n",
            "4763  [ 0.  0. -1. -1.  0. -1.  1.  1.  0.  0.  0.  ...   0.00\n",
            "\n",
            "[4764 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaziVPOZQDCR"
      },
      "outputs": [],
      "source": [
        "import pickle #Se utiliza para serializar y deserializar objetos en Python.\n",
        "\n",
        "with open('agente.pickle', 'wb') as handle: #abre un archivo llamado en modo escritura binaria.\n",
        "    pickle.dump(Agent1.value_function, handle, protocol=pickle.HIGHEST_PROTOCOL) # guarda el objeto Agent1 en el archivo abierto utilizando la función pickle.dump"
      ]
    }
  ]
}